{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvlZmynBUKAG"
      },
      "outputs": [],
      "source": [
        "#data cleaning \n",
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
        "    return text\n",
        "\n",
        "def tokenize_text(text):\n",
        "    return text.split()\n",
        "\n",
        "input_text = \"This is a sample input text for summarization.\"\n",
        "summary_text = \"This is a summary of the input text.\"\n",
        "\n",
        "clean_input_text = clean_text(input_text)\n",
        "clean_summary_text = clean_text(summary_text)\n",
        "\n",
        "tokenized_input_text = tokenize_text(clean_input_text)\n",
        "tokenized_summary_text = tokenize_text(clean_summary_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkrYfDOgUOgz"
      },
      "outputs": [],
      "source": [
        "# Word Sense Disambiguation (WSD)\n",
        "# For WSD, we can use various pre-trained models such as WordNet or Lesk algorithm. Here's an example code using the Lesk algorithm for WSD:\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def wsd_sentence(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    disambiguated_sentence = []\n",
        "    for token in tokens:\n",
        "        synset = lesk(tokens, token)\n",
        "        if synset is not None:\n",
        "            disambiguated_sentence.append(synset.name())\n",
        "    return \" \".join(disambiguated_sentence)\n",
        "\n",
        "disambiguated_input_text = wsd_sentence(input_text)\n",
        "disambiguated_summary_text = wsd_sentence(summary_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acZJ_bglUVcH"
      },
      "outputs": [],
      "source": [
        "# Sequence-to-Sequence (Seq2Seq) Model\n",
        "# For implementing a Seq2Seq model, we can use various libraries such as TensorFlow, PyTorch, or Keras. Here's an example code for a simple Seq2Seq model in Keras that incorporates WSD:\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding\n",
        "\n",
        "# Define input and output sequences\n",
        "input_seq = Input(shape=(None,))\n",
        "output_seq = Input(shape=(None,))\n",
        "\n",
        "# Define embedding layer\n",
        "vocab_size = 10000\n",
        "embedding_size = 100\n",
        "embed_layer = Embedding(vocab_size, embedding_size)\n",
        "\n",
        "# Define encoder and decoder layers\n",
        "encoder_lstm = LSTM(256, return_state=True)\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "dense_layer = Dense(vocab_size, activation='softmax')\n",
        "\n",
        "# Define encoder model\n",
        "embed_input = embed_layer(input_seq)\n",
        "encoder_output, state_h, state_c = encoder_lstm(embed_input)\n",
        "encoder_states = [state_h, state_c]\n",
        "encoder_model = Model(input_seq, encoder_states)\n",
        "\n",
        "# Define decoder model\n",
        "embed_output = embed_layer(output_seq)\n",
        "decoder_output, _, _ = decoder_lstm(embed_output, initial_state=encoder_states)\n",
        "decoder_output = dense_layer(decoder_output)\n",
        "decoder_model = Model([output_seq] + encoder_states, [decoder_output])\n",
        "\n",
        "# Define Seq2Seq model\n",
        "seq2seq_output = decoder_model([output_seq] + encoder_model(input_seq))\n",
        "seq2seq_model = Model([input_seq, output_seq], seq2seq_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvEzAlMIUdCp"
      },
      "outputs": [],
      "source": [
        "# In this code, we first load the pre-trained BERT model and tokenizer using the AutoTokenizer and TFAutoModel classes from the transformers library. \n",
        "# Then, we define the input and target texts and encode them using the tokenizer, producing input and target encodings in the form of tensors.\n",
        "#  We pass these tensors to the BERT model to obtain the encoded representations of the input and target texts.\n",
        "# Next, we use the encoded representations to generalize the content of the input and target texts by taking the mean of the embeddings \n",
        "# along the sequence length dimension. We then use the top_k function to filter out the top 10 most important embeddings, and use the gather \n",
        "# function to extract the corresponding embeddings from the input and target encoded representations.\n",
        "# Overall, this code demonstrates how to use a pre-trained language model like BERT to encode and generalize the \n",
        "# content of input and target texts, and how to use the encoded representations to filter out irrelevant information.\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "bert = TFAutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Define input and target texts\n",
        "input_texts = ['This is a sample input text.', 'Another input text.']\n",
        "target_texts = ['This is a summary of the first input text.', 'Summary of the second input text.']\n",
        "\n",
        "# Encode the input and target texts using BERT\n",
        "input_encodings = tokenizer(input_texts, truncation=True, padding=True, return_tensors='tf')\n",
        "target_encodings = tokenizer(target_texts, truncation=True, padding=True, return_tensors='tf')\n",
        "\n",
        "input_embeddings = bert(input_encodings['input_ids'])[0]\n",
        "target_embeddings = bert(target_encodings['input_ids'])[0]\n",
        "\n",
        "# Use the encoded representations to generalize the content\n",
        "input_summary = tf.reduce_mean(input_embeddings, axis=1)\n",
        "target_summary = tf.reduce_mean(target_embeddings, axis=1)\n",
        "\n",
        "# Filter out irrelevant information\n",
        "input_filter = tf.math.top_k(input_summary, k=10)\n",
        "target_filter = tf.math.top_k(target_summary, k=10)\n",
        "\n",
        "input_summary = tf.gather(input_embeddings, input_filter.indices, batch_dims=1)\n",
        "target_summary = tf.gather(target_embeddings, target_filter.indices, batch_dims=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBk6Ihn-WWnd"
      },
      "outputs": [],
      "source": [
        "# To fine-tune a Sequence-to-Sequence model on a large dataset of input-output text pairs and evaluate the quality of the generated summaries, \n",
        "# we can use a loss function that combines metrics such as ROUGE and BLEU. Here's an example code in Python using the tensorflow and nltk libraries:\n",
        "\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge import Rouge\n",
        "\n",
        "# Define the loss function\n",
        "def combined_loss(y_true, y_pred):\n",
        "    # Convert the predicted and target summaries to text\n",
        "    target_seq = y_true[:, 1:]\n",
        "    pred_seq = tf.argmax(y_pred[:, :-1, :], axis=-1)\n",
        "    target_text = [tokenizer.decode(seq) for seq in target_seq]\n",
        "    pred_text = [tokenizer.decode(seq) for seq in pred_seq]\n",
        "\n",
        "    # Calculate the ROUGE and BLEU scores\n",
        "    rouge = Rouge()\n",
        "    rouge_scores = rouge.get_scores(pred_text, target_text)\n",
        "    rouge_score = sum([score['rouge-l']['f'] for score in rouge_scores]) / len(rouge_scores)\n",
        "\n",
        "    bleu_scores = [sentence_bleu([ref.split()], hyp.split()) for ref, hyp in zip(target_text, pred_text)]\n",
        "    bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "    # Combine the ROUGE and BLEU scores to obtain the loss\n",
        "    alpha = 0.5\n",
        "    loss = alpha * (1 - rouge_score) + (1 - alpha) * (1 - bleu_score)\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Load the dataset of input-output text pairs\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_texts, target_texts))\n",
        "\n",
        "# Tokenize the input and output sequences\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(input_texts + target_texts)\n",
        "\n",
        "input_seqs = tokenizer.texts_to_sequences(input_texts)\n",
        "target_seqs = tokenizer.texts_to_sequences(target_texts)\n",
        "\n",
        "# Pad the input and output sequences to a fixed length\n",
        "max_seq_len = 128\n",
        "input_seqs = tf.keras.preprocessing.sequence.pad_sequences(input_seqs, maxlen=max_seq_len, padding='post')\n",
        "target_seqs = tf.keras.preprocessing.sequence.pad_sequences(target_seqs, maxlen=max_seq_len, padding='post')\n",
        "\n",
        "# Define the Seq2Seq model architecture\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(max_seq_len,))\n",
        "encoder_embedding = tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = tf.keras.layers.LSTM(hidden_dim, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(max_seq_len,))\n",
        "decoder_embedding = tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM(hidden_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = tf.keras.layers.Dense(len(tokenizer.word_index) + 1, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Compile the model and fit it to the dataset\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss=combined_loss)\n",
        "\n",
        "model.fit([input_seqs, target_seqs[:, :-1]], target_seqs[:, 1:], batch_size=32, epochs=10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkQUDV7PXd6F"
      },
      "outputs": [],
      "source": [
        "# To test the trained Sequence-to-Sequence model on a held-out dataset and fine-tune the model as needed to improve its performance, we can use the following code in Python:\n",
        "# Load the held-out dataset of input-output text pairs\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_input_texts, test_target_texts))\n",
        "\n",
        "# Tokenize the input and output sequences\n",
        "test_input_seqs = tokenizer.texts_to_sequences(test_input_texts)\n",
        "test_target_seqs = tokenizer.texts_to_sequences(test_target_texts)\n",
        "\n",
        "# Pad the input and output sequences to a fixed length\n",
        "test_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(test_input_seqs, maxlen=max_seq_len, padding='post')\n",
        "test_target_seqs = tf.keras.preprocessing.sequence.pad_sequences(test_target_seqs, maxlen=max_seq_len, padding='post')\n",
        "\n",
        "# Evaluate the model on the held-out dataset\n",
        "scores = model.evaluate([test_input_seqs, test_target_seqs[:, :-1]], test_target_seqs[:, 1:], verbose=0)\n",
        "print('Test loss:', scores)\n",
        "\n",
        "# Generate summaries for a few input texts and compare with the target summaries\n",
        "for i in range(10):\n",
        "    input_seq = input_seqs[i:i+1]\n",
        "    target_seq = target_seqs[i:i+1, 1:]\n",
        "    pred_seq = model.predict([input_seq, target_seq], verbose=0)\n",
        "    pred_text = tokenizer.decode(tf.squeeze(tf.argmax(pred_seq, axis=-1)))\n",
        "    target_text = tokenizer.decode(tf.squeeze(target_seq))\n",
        "    print('Input text:', input_texts[i])\n",
        "    print('Predicted summary:', pred_text)\n",
        "    print('Target summary:', target_text)\n",
        "    print()\n",
        "# In this code, we load the held-out dataset of input-output text pairs and tokenize the input and output sequences. We then pad the sequences to a fixed length and \n",
        "# evaluate the model on the held-out dataset using the evaluate method. We print the test loss to see how well the model performs on the held-out dataset.\n",
        "# We also generate summaries for a few input texts using the predict method of the model and compare the predicted summaries with the target summaries. \n",
        "# We print the input text, predicted summary, and target summary to see how well the model performs on these examples.\n",
        "# Based on the test loss and the quality of the generated summaries, we can fine-tune the model as needed to improve its performance. \n",
        "# This may involve adjusting the model architecture, hyperparameters, or loss function, or collecting more training data."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "f55e8242e5c11214fcb75b1d7986b94ac721889407cfcdcff58a06358a89c17c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
