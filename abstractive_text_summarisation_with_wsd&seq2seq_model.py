# -*- coding: utf-8 -*-
"""Abstractive_Text_Summarisation_with_wsd&seq2seq model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1095Ft0PjJm8hyvoQjzvTaSbwJ6lWxI5g
"""

!pip install tensorflow nltk datasets
!pip install --upgrade tensorflow

import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, TimeDistributed
from tensorflow.keras.models import Model
import nltk
from datasets import load_dataset
nltk.download('wordnet')
nltk.download('omw-1.4')

dataset = load_dataset("cnn_dailymail", "3.0.0")
train_data = dataset['train']

import nltk

# Download the 'punkt' resource
nltk.download('punkt')

from nltk.corpus import wordnet as wn
from nltk.tokenize import word_tokenize

def disambiguate_word_senses(sentence):
    words = word_tokenize(sentence)
    lemmatized_sentence = []
    for word in words:
        synsets = wn.synsets(word)
        if synsets:
            lemmatized_sentence.append(synsets[0].lemmas()[0].name())  # Choose the most common sense
        else:
            lemmatized_sentence.append(word)
    return ' '.join(lemmatized_sentence)

# Assuming train_data is a list of dictionaries with key 'article'
preprocessed_texts = [disambiguate_word_senses(article['article']) for article in train_data if 'article' in article]

preprocessed_texts

import pickle
import os

# Save preprocessed_text
preprocessed_texts_path = r"/Desktop/Abstractive Text Summarisation Minor Project"

# Function for saving model, variable for another runtime
def save_work(path, var_str, var_name):
    temp = os.path.join(path, var_name)

    # Create the directory if it doesn't exist
    os.makedirs(os.path.dirname(temp), exist_ok=True)

    absolute_path = os.path.abspath(temp)
    print(f"Saving file to: {absolute_path}")

    try:
        if not os.path.exists(temp):
            with open(temp, 'wb') as f:
                pickle.dump(var_str, f)
            print("File saved successfully.")
            return True
        else:
            print("File already exists.")
            return False
    except Exception as e:
        print(f"An error occurred: {e}")
        return False

# Function for loading model, variable for another runtime
def load_work(path,var_name):
    temp = os.path.join(path,var_name)
    if os.path.exists(temp):
        with open(temp, 'rb') as f:
            var_name = pickle.load(f)
        return var_name
    else:
        return False

save_work(preprocessed_texts_path,preprocessed_texts,var_name="preprocessed_texts")

preprocessed_texts = load_work(preprocessed_texts_path,var_name = "preprocessed_texts")

preprocessed_texts

# Model parameters
vocab_size = 10000  # Choose your vocabulary size
embedding_dim = 256
lstm_units = 512

# Encoder
encoder_inputs = Input(shape=(None,))
enc_emb = Embedding(vocab_size, embedding_dim)(encoder_inputs)
encoder_lstm = LSTM(lstm_units, return_state=True)
encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)
encoder_states = [state_h, state_c]

# Decoder
decoder_inputs = Input(shape=(None,))
dec_emb_layer = Embedding(vocab_size, embedding_dim)
dec_emb = dec_emb_layer(decoder_inputs)
decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)
decoder_dense = TimeDistributed(Dense(vocab_size, activation='softmax'))
decoder_outputs = decoder_dense(decoder_outputs)

# Seq2Seq Model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

# Model Parameters
vocab_size = 6000  # Define your vocabulary size
max_seq_length = 350  # Define the maximum sequence length
embedding_dim = 256
lstm_units = 512

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

# Initialize the tokenizer
tokenizer = Tokenizer(num_words=vocab_size, oov_token='<unk>')
tokenizer.fit_on_texts(preprocessed_texts)

# Convert texts to sequences
sequences = tokenizer.texts_to_sequences(preprocessed_texts)

# Determine the desired sequence length (you can set this to a fixed value)
max_seq_length = 100

# Pad or truncate sequences to the desired length
encoder_input_data = pad_sequences(sequences, maxlen=max_seq_length, padding='post')

# Prepare decoder input data and output data
# Shift target sequences by one time step
decoder_input_data = sequences  # Decoder input is the same as target, without the last word
decoder_target_data = [seq[1:] for seq in sequences]  # Decoder target is one step ahead

# Pad or truncate decoder sequences to the same length as encoder sequences
decoder_input_data = pad_sequences(decoder_input_data, maxlen=max_seq_length, padding='post')
decoder_target_data = pad_sequences(decoder_target_data, maxlen=max_seq_length, padding='post')

# Convert lists to NumPy arrays
encoder_input_data = np.array(encoder_input_data)
decoder_input_data = np.array(decoder_input_data)
decoder_target_data = np.array(decoder_target_data)  # Ensure this is an array of integers

# Compile the model
model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')

epochs = 3

# Set batch size
batch_size = 64

# Calculate the total number of samples
total_samples = len(encoder_input_data)

# Determine the number of batches
num_batches = total_samples // batch_size

from tqdm import tqdm

# Calculate the total number of batches including the remaining samples
total_batches = (total_samples + batch_size - 1) // batch_size

# Train the model in batches
for epoch in range(epochs):
    print(f'Epoch {epoch + 1}/{epochs}')

    # Create tqdm progress bar for batches
    progress_bar = tqdm(range(total_batches), desc=f'Epoch {epoch + 1}/{epochs}')

    for batch in progress_bar:
        start_idx = batch * batch_size
        end_idx = min((batch + 1) * batch_size, total_samples)
        encoder_batch_data = encoder_input_data[start_idx:end_idx]
        decoder_input_batch = decoder_input_data[start_idx:end_idx]
        decoder_target_batch = decoder_target_data[start_idx:end_idx]

        # Train the model on the current batch
        loss = model.train_on_batch([encoder_batch_data, decoder_input_batch], decoder_target_batch)

        # Update the progress bar description with the current loss
        progress_bar.set_postfix(loss=loss, refresh=True)

model_path = r'/Desktop/Abstractive Text Summarisation Minor Project'

save_work(model_path,model,var_name='abstract_text_summarision_pickle_model')

model_pickel = load_work(model_path,var_name='abstract_text_summarision_pickle_model')

model_pickel

model.save(r'/Desktop/Abstractive Text Summarisation Minor Project/ATS_model_h5_.h5')

from tensorflow.keras.models import load_model

# Specify the full path to your saved model
model_path_h5_local = r'/Desktop/Abstractive Text Summarisation Minor Project/ATS_model_h5_.h5'

# Load the model
model_h5 = load_model(model_path_h5_local)

model_pickel

model_h5

model

# Define the encoder model
encoder_model = Model(encoder_inputs, encoder_states)

# Define the decoder model
decoder_state_input_h = Input(shape=(lstm_units,))
decoder_state_input_c = Input(shape=(lstm_units,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

dec_emb2 = dec_emb_layer(decoder_inputs)
decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)
decoder_states2 = [state_h2, state_c2]
decoder_outputs2 = decoder_dense(decoder_outputs2)

decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)

def summarize_text(input_text):
    # Convert input_text to sequences and pad
    input_seq = tokenizer.texts_to_sequences([input_text])
    input_seq = pad_sequences(input_seq, maxlen=max_seq_length, padding='post')

    # Encode input and retrieve initial decoder state
    states_value = encoder_model.predict(input_seq)

    # Generate empty target sequence of length 1
    target_seq = np.zeros((1, 1))
    # Populate the first character of target sequence with the start character
    target_seq[0, 0] = tokenizer.word_index['start']

    stop_condition = False
    decoded_sentence = ''

    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)

        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = tokenizer.index_word[sampled_token_index]
        decoded_sentence += ' ' + sampled_char

        # Exit condition: either hit max length or find stop character
        if (sampled_char == 'end' or len(decoded_sentence) > 50):
            stop_condition = True

        # Update the target sequence (of length 1)
        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = sampled_token_index

        # Update states
        states_value = [h, c]

    return decoded_sentence.strip()

input_article = '''As results for assembly elections in four states - Chhattisgarh, Rajasthan, Madhya Pradesh and Telangana - pour in, the BJP looks to score a big win in three heartland states. In Telangana, K Chandrasekhar Rao-led BRS has accepted defeat against the Congress.
Prime Minister Narendra Modi is addressing the party workers at the party headquarters in Delhi shortly

BJP leaders credited Prime Minister Narendra Modi's leadership, Amit Shah's strategy and party's welfare policies for the positive trends that show a clear victory for the party in three states. The opposition, however, claims that the results will not have any impact on the Lok Sabha elections in 2024.  '''
# This is the input article that you want to summarize.

# You can use the summarize function to generate a summary:
summary = summarize_text(input_article)

print("Generated Summary:", summary)